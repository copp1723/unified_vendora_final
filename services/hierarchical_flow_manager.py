"""
VENDORA Hierarchical Flow Manager - Fixed Implementation
Implements the complete L1 -> L2 -> L3 -> L1 agent hierarchy with proper error handling
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import uuid
from abc import ABC, abstractmethod
import hashlib
from tenacity import retry, stop_after_attempt, wait_exponential
import google.generativeai as genai
from google.cloud import bigquery
from google.oauth2 import service_account

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TaskComplexity(Enum):
    """Task complexity levels"""
    SIMPLE = "simple"      # Basic KPI lookup
    STANDARD = "standard"  # Standard analysis
    COMPLEX = "complex"    # Complex forecasting, multi-year analysis
    CRITICAL = "critical"  # High-stakes decision support


class InsightStatus(Enum):
    """Insight generation status"""
    PENDING = "pending"
    ANALYZING = "analyzing"
    GENERATING = "generating"
    VALIDATING = "validating"
    APPROVED = "approved"
    REJECTED = "rejected"
    REVISION_NEEDED = "revision_needed"
    DELIVERED = "delivered"


@dataclass
class AnalyticalTask:
    """Represents a user query transformed into an analytical task"""
    id: str
    user_query: str
    complexity: TaskComplexity
    dealership_id: str
    required_data: List[str] = field(default_factory=list)
    context: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    assigned_specialist: Optional[str] = None
    
    def __post_init__(self):
        if not self.id:
            self.id = f"TASK-{uuid.uuid4().hex[:8]}"


@dataclass
class DraftInsight:
    """Draft insight generated by L2 specialist agents"""
    task_id: str
    agent_id: str
    content: Dict[str, Any]
    sql_queries: List[str] = field(default_factory=list)
    data_sources: List[str] = field(default_factory=list)
    confidence_score: float = 0.0
    generation_time_ms: int = 0
    methodology: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "agent_id": self.agent_id,
            "content": self.content,
            "sql_queries": self.sql_queries,
            "data_sources": self.data_sources,
            "confidence_score": self.confidence_score,
            "generation_time_ms": self.generation_time_ms,
            "methodology": self.methodology
        }


@dataclass
class ValidatedInsight:
    """Final validated insight approved by L3 Master Analyst"""
    task_id: str
    draft_insight: DraftInsight
    validation_result: Dict[str, Any]
    final_content: Dict[str, Any]
    quality_score: float
    approval_timestamp: datetime = field(default_factory=datetime.now)
    revision_count: int = 0
    
    def to_user_response(self) -> Dict[str, Any]:
        """Convert to user-friendly response format"""
        return {
            "insight": self.final_content,
            "confidence": self.quality_score,
            "data_sources": self.draft_insight.data_sources,
            "generated_at": self.approval_timestamp.isoformat()
        }


@dataclass
class FlowState:
    """Tracks the state of a task through the hierarchy"""
    task: AnalyticalTask
    status: InsightStatus = InsightStatus.PENDING
    current_level: int = 1
    draft_insights: List[DraftInsight] = field(default_factory=list)
    validated_insight: Optional[ValidatedInsight] = None
    error_log: List[Dict[str, Any]] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    
    def add_error(self, level: int, error: str, details: Dict[str, Any] = None):
        """Log an error in the flow"""
        self.error_log.append({
            "level": level,
            "error": error,
            "details": details or {},
            "timestamp": datetime.now().isoformat()
        })
    
    def get_duration_ms(self) -> int:
        """Get total processing duration"""
        return int((datetime.now() - self.start_time).total_seconds() * 1000)


class HierarchicalFlowManager:
    """
    Manages the complete L1 -> L2 -> L3 -> L1 hierarchical flow for VENDORA
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.flows: Dict[str, FlowState] = {}
        self.orchestrator = None  # L1
        self.specialists: Dict[str, Any] = {}  # L2
        self.master_analyst = None  # L3
        self.gemini_model = None  # Gemini API model
        self.bigquery_client = None  # BigQuery client
        self.query_cache: Dict[str, Any] = {}  # Simple in-memory cache
        self.metrics = {
            "total_queries": 0,
            "approved_insights": 0,
            "rejected_insights": 0,
            "avg_processing_time_ms": 0,
            "complexity_distribution": {}
        }
        
    async def initialize(self):
        """Initialize all components of the hierarchy"""
        logger.info("ðŸš€ Initializing Hierarchical Flow Manager")
        
        try:
            # Initialize clients first
            await self._initialize_clients()
            
            # Initialize L1 - Orchestrator
            from .vendora_orchestrator import VendoraOrchestrator
            self.orchestrator = VendoraOrchestrator(self.config)
            await self.orchestrator.initialize()
            
            # Initialize L2 - Specialist Agents
            from .vendora_specialists import DataAnalystAgent, SeniorAnalystAgent
            self.specialists = {
                "data_analyst": DataAnalystAgent(self.config),
                "senior_analyst": SeniorAnalystAgent(self.config)
            }
            
            for name, agent in self.specialists.items():
                await agent.initialize()
                logger.info(f"âœ… Initialized L2 specialist: {name}")
            
            # Initialize L3 - Master Analyst
            from .vendora_master_analyst import MasterAnalyst
            self.master_analyst = MasterAnalyst(self.config)
            await self.master_analyst.initialize()
            
            logger.info("âœ… Hierarchical Flow Manager initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize flow manager: {str(e)}", exc_info=True)
            raise
    
    async def _initialize_clients(self):
        """Initialize external service clients with proper error handling"""
        logger.info("Initializing external clients...")
        
        # Initialize Gemini
        try:
            genai.configure(api_key=self.config.get("gemini_api_key"))
            self.gemini_model = genai.GenerativeModel(
                'gemini-pro',
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": 2048,
                    "top_k": 40,
                    "top_p": 0.95,
                }
            )
            logger.info("âœ… Gemini API initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini: {str(e)}")
            raise
        
        # Initialize BigQuery
        try:
            if self.config.get('service_account_path'):
                credentials = service_account.Credentials.from_service_account_file(
                    self.config['service_account_path']
                )
                self.bigquery_client = bigquery.Client(
                    credentials=credentials,
                    project=self.config.get('bigquery_project')
                )
            else:
                # Use default credentials
                self.bigquery_client = bigquery.Client(
                    project=self.config.get('bigquery_project')
                )
            
            # Test connection
            self.bigquery_client.query("SELECT 1").result()
            logger.info("âœ… BigQuery client initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery: {str(e)}")
            # Continue without BigQuery for development
            logger.warning("Continuing without BigQuery - using mock data mode")
            self.bigquery_client = None
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def process_user_query(self, user_query: str, dealership_id: str, 
                                user_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Main entry point - processes a user query through the complete hierarchy
        
        Flow: User -> L1 (Orchestrator) -> L2 (Specialist) -> L3 (Master) -> L1 -> User
        """
        self.metrics["total_queries"] += 1
        
        # Validate inputs
        if not user_query or not user_query.strip():
            return {"error": "Query cannot be empty"}
        
        if not dealership_id or not dealership_id.replace('_', '').replace('-', '').isalnum():
            return {"error": "Invalid dealership ID format"}
        
        # Create analytical task
        task = AnalyticalTask(
            id=f"TASK-{uuid.uuid4().hex[:8]}",
            user_query=user_query.strip(),
            dealership_id=dealership_id,
            complexity=TaskComplexity.STANDARD,  # Will be updated by L1
            context=user_context or {}
        )
        
        # Initialize flow state
        flow_state = FlowState(task=task)
        self.flows[task.id] = flow_state
        
        try:
            # Check cache first
            cache_key = self._get_cache_key(user_query, dealership_id)
            if cache_key in self.query_cache:
                cached_result = self.query_cache[cache_key]
                logger.info(f"Returning cached result for task {task.id}")
                cached_result["metadata"]["cached"] = True
                return cached_result
            
            # L1: Task ingestion and intelligent dispatch
            logger.info(f"ðŸ“¥ L1: Processing query for task {task.id}")
            flow_state.status = InsightStatus.ANALYZING
            
            dispatch_result = await self.orchestrator.analyze_and_dispatch(task)
            task.complexity = dispatch_result["complexity"]
            task.required_data = dispatch_result["required_data"]
            task.assigned_specialist = dispatch_result["assigned_agent"]
            
            self.metrics["complexity_distribution"][task.complexity.value] = \
                self.metrics["complexity_distribution"].get(task.complexity.value, 0) + 1
            
            # L2: Analysis and insight generation
            logger.info(f"ðŸ”¬ L2: Specialist {task.assigned_specialist} analyzing task {task.id}")
            flow_state.current_level = 2
            flow_state.status = InsightStatus.GENERATING
            
            specialist = self.specialists.get(task.assigned_specialist)
            if not specialist:
                raise ValueError(f"Specialist {task.assigned_specialist} not found")
                
            draft_insight = await specialist.generate_insight(task)
            flow_state.draft_insights.append(draft_insight)
            
            # L3: Validation and quality gate
            logger.info(f"âœ… L3: Master Analyst validating insight for task {task.id}")
            flow_state.current_level = 3
            flow_state.status = InsightStatus.VALIDATING
            
            validation_result = await self.master_analyst.validate_insight(
                task, draft_insight
            )
            
            # Handle validation result
            if validation_result["approved"]:
                # Create validated insight
                validated_insight = ValidatedInsight(
                    task_id=task.id,
                    draft_insight=draft_insight,
                    validation_result=validation_result,
                    final_content=validation_result["final_content"],
                    quality_score=validation_result["quality_score"]
                )
                
                flow_state.validated_insight = validated_insight
                flow_state.status = InsightStatus.APPROVED
                self.metrics["approved_insights"] += 1
                
                # L1: Deliver to user
                logger.info(f"ðŸ“¤ L1: Delivering approved insight for task {task.id}")
                flow_state.current_level = 1
                flow_state.status = InsightStatus.DELIVERED
                
                response = await self.orchestrator.format_user_response(
                    validated_insight
                )
                
            else:
                # Handle rejection/revision
                if validation_result.get("needs_revision") and len(flow_state.draft_insights) < 3:
                    flow_state.status = InsightStatus.REVISION_NEEDED
                    
                    # Send back to L2 for revision
                    logger.info(f"ðŸ”„ Sending task {task.id} back for revision")
                    
                    # Revision with improved feedback
                    revised_insight = await specialist.revise_insight(
                        task, draft_insight, validation_result["feedback"]
                    )
                    flow_state.draft_insights.append(revised_insight)
                    
                    # Re-validate
                    validation_result = await self.master_analyst.validate_insight(
                        task, revised_insight
                    )
                    
                    if validation_result["approved"]:
                        validated_insight = ValidatedInsight(
                            task_id=task.id,
                            draft_insight=revised_insight,
                            validation_result=validation_result,
                            final_content=validation_result["final_content"],
                            quality_score=validation_result["quality_score"],
                            revision_count=1
                        )
                        
                        flow_state.validated_insight = validated_insight
                        flow_state.status = InsightStatus.APPROVED
                        self.metrics["approved_insights"] += 1
                        
                        response = await self.orchestrator.format_user_response(
                            validated_insight
                        )
                    else:
                        # Final rejection
                        flow_state.status = InsightStatus.REJECTED
                        self.metrics["rejected_insights"] += 1
                        response = {
                            "error": "Unable to generate validated insight after revision",
                            "task_id": task.id,
                            "suggestions": [
                                "Try rephrasing your query",
                                "Make your question more specific",
                                "Check if the requested data is available"
                            ]
                        }
                else:
                    # Direct rejection
                    flow_state.status = InsightStatus.REJECTED
                    self.metrics["rejected_insights"] += 1
                    response = {
                        "error": validation_result.get("reason", "Insight validation failed"),
                        "task_id": task.id,
                        "quality_score": validation_result.get("quality_score", 0)
                    }
            
            # Update metrics
            processing_time = flow_state.get_duration_ms()
            self._update_avg_processing_time(processing_time)
            
            # Add metadata to response
            response["metadata"] = {
                "task_id": task.id,
                "complexity": task.complexity.value,
                "processing_time_ms": processing_time,
                "revision_count": flow_state.validated_insight.revision_count 
                    if flow_state.validated_insight else 0,
                "cached": False
            }
            
            # Cache successful results
            if flow_state.status == InsightStatus.DELIVERED:
                self.query_cache[cache_key] = response
            
            return response
            
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ Timeout processing task {task.id}")
            flow_state.add_error(
                flow_state.current_level,
                "Processing timeout",
                {"timeout": "30s"}
            )
            flow_state.status = InsightStatus.REJECTED
            
            return {
                "error": "Query processing timed out",
                "task_id": task.id,
                "suggestions": [
                    "Try a simpler query",
                    "Reduce the time range",
                    "Be more specific in your request"
                ]
            }
            
        except Exception as e:
            logger.error(f"âŒ Error processing task {task.id}: {str(e)}", exc_info=True)
            flow_state.add_error(
                flow_state.current_level,
                str(e),
                {"type": type(e).__name__}
            )
            flow_state.status = InsightStatus.REJECTED
            
            return {
                "error": "An error occurred processing your query",
                "task_id": task.id,
                "message": "Please try again or contact support if the issue persists"
            }
    
    def _get_cache_key(self, query: str, dealership_id: str) -> str:
        """Generate cache key for query results"""
        cache_data = f"{query.lower().strip()}:{dealership_id}"
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    def _update_avg_processing_time(self, new_time_ms: int):
        """Update rolling average processing time"""
        current_avg = self.metrics["avg_processing_time_ms"]
        total_queries = self.metrics["total_queries"]
        
        # Calculate new average
        self.metrics["avg_processing_time_ms"] = (
            (current_avg * (total_queries - 1) + new_time_ms) / total_queries
        )
    
    async def get_flow_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get the current status of a flow"""
        flow_state = self.flows.get(task_id)
        if not flow_state:
            return None
        
        return {
            "task_id": task_id,
            "status": flow_state.status.value,
            "current_level": flow_state.current_level,
            "complexity": flow_state.task.complexity.value,
            "duration_ms": flow_state.get_duration_ms(),
            "draft_insights_count": len(flow_state.draft_insights),
            "has_validated_insight": flow_state.validated_insight is not None,
            "error_count": len(flow_state.error_log),
            "errors": flow_state.error_log[-3:] if flow_state.error_log else []  # Last 3 errors
        }
    
    async def get_metrics(self) -> Dict[str, Any]:
        """Get system metrics"""
        active_flows = [
            f for f in self.flows.values() 
            if f.status not in [InsightStatus.DELIVERED, InsightStatus.REJECTED]
        ]
        
        return {
            **self.metrics,
            "active_flows": len(active_flows),
            "total_flows": len(self.flows),
            "approval_rate": self.metrics["approved_insights"] / self.metrics["total_queries"] 
                           if self.metrics["total_queries"] > 0 else 0,
            "cache_size": len(self.query_cache),
            "cache_hit_rate": "Not tracked"  # Would need to implement hit tracking
        }
    
    async def shutdown(self):
        """Gracefully shutdown the flow manager"""
        logger.info("ðŸ›‘ Shutting down Hierarchical Flow Manager")
        
        # Wait for active flows to complete (with timeout)
        active_flows = [
            flow for flow in self.flows.values()
            if flow.status not in [InsightStatus.DELIVERED, InsightStatus.REJECTED]
        ]
        
        if active_flows:
            logger.info(f"Waiting for {len(active_flows)} active flows to complete...")
            
            # Give flows 10 seconds to complete
            await asyncio.sleep(10)
            
            # Force complete remaining flows
            for flow in active_flows:
                flow.status = InsightStatus.REJECTED
                flow.add_error(0, "Shutdown forced", {"reason": "System shutdown"})
        
        # Shutdown components
        if self.orchestrator:
            await self.orchestrator.shutdown()
        
        for agent in self.specialists.values():
            await agent.shutdown()
        
        if self.master_analyst:
            await self.master_analyst.shutdown()
        
        # Close clients
        if self.bigquery_client:
            self.bigquery_client.close()
        
        # Log final metrics
        logger.info(f"Final metrics: {json.dumps(self.metrics, indent=2)}")
        
        logger.info("âœ… Hierarchical Flow Manager shutdown complete")


# Example usage
if __name__ == "__main__":
    async def main():
        # Configuration
        config = {
            "gemini_api_key": "your-api-key",
            "bigquery_project": "your-project",
            "quality_threshold": 0.9
        }
        
        # Create flow manager
        flow_manager = HierarchicalFlowManager(config)
        await flow_manager.initialize()
        
        # Test query
        result = await flow_manager.process_user_query(
            user_query="What were my top selling vehicles last month?",
            dealership_id="dealer_123",
            user_context={"user_role": "sales_manager"}
        )
        
        print(json.dumps(result, indent=2))
        
        # Get metrics
        metrics = await flow_manager.get_metrics()
        print(f"\nMetrics: {json.dumps(metrics, indent=2)}")
        
        # Shutdown
        await flow_manager.shutdown()
    
    asyncio.run(main())
